{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee7f3f6fa5041fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.252164729Z",
     "start_time": "2024-02-06T12:24:43.694825996Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import holidays\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tomllib\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pathlib\n",
    "from typing import Generator\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa35ef398085195",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Environment variables and Constants definition"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "SEED = 42\n",
    "DATE_FORMAT: str = '%Y-%m-%d'\n",
    "DATETIME_FORMAT: str = '%Y-%m-%d %H:%M:%S'\n",
    "TIMEZONE: str = \"Europe/Tallinn\"\n",
    "ESTONIAN_HOLIDAYS = list(\n",
    "    holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n",
    ")\n",
    "\n",
    "MAP_MODEL_TYPE: dict[str, int] = {\n",
    "    \"producer\": 0,\n",
    "    \"consumer\": 1\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.298754853Z",
     "start_time": "2024-02-06T12:24:44.254364843Z"
    }
   },
   "id": "fc1af53ccd3bef24",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "37e0015b125fead6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pl.set_random_seed(seed)\n",
    "\n",
    "\n",
    "set_seed(seed=SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.300122334Z",
     "start_time": "2024-02-06T12:24:44.290570141Z"
    }
   },
   "id": "d272fe31ca6ba57f",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Paths"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5325d69d614419b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaea117aee62205"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BASE_DATA_PATH: pathlib.Path = pathlib.Path().absolute().parent.parent / 'data' / 'raw'\n",
    "\n",
    "CLIENTS_PATH: pathlib.Path = BASE_DATA_PATH / 'client.csv'\n",
    "ELECTRICITY_PATH: pathlib.Path = BASE_DATA_PATH / 'electricity_prices.csv'\n",
    "GAS_PATH: pathlib.Path = BASE_DATA_PATH / 'gas_prices.csv'\n",
    "HISTORICAL_WEATHER_PATH: pathlib.Path = BASE_DATA_PATH / 'historical_weather.csv'\n",
    "WEATHER_FORECAST_PATH: pathlib.Path = BASE_DATA_PATH / 'forecast_weather.csv'\n",
    "TRAIN_PATH: pathlib.Path = BASE_DATA_PATH / 'train.csv'\n",
    "WEATHER_STATION_COUNTY_PATH: pathlib.Path = BASE_DATA_PATH / 'weather_station_to_county_mapping.csv'\n",
    "COUNTY_MAP_PATH: pathlib.Path = BASE_DATA_PATH / \"county_id_to_name_map.json\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.301366914Z",
     "start_time": "2024-02-06T12:24:44.293273695Z"
    }
   },
   "id": "592739ccc636e342",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test data path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c713599df8f1f4c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "TEST_DATA_PATH: pathlib.Path = BASE_DATA_PATH / \"example_test_files\"\n",
    "\n",
    "CLIENTS_TEST_PATH: pathlib.Path = TEST_DATA_PATH / 'client.csv'\n",
    "ELECTRICITY_TEST_PATH: pathlib.Path = TEST_DATA_PATH / 'electricity_prices.csv'\n",
    "GAS_TEST_PATH: pathlib.Path = TEST_DATA_PATH / 'gas_prices.csv'\n",
    "HISTORICAL_TEST_WEATHER_PATH: pathlib.Path = TEST_DATA_PATH / 'historical_weather.csv'\n",
    "WEATHER_FORECAST_TEST_PATH: pathlib.Path = TEST_DATA_PATH / 'forecast_weather.csv'\n",
    "TRAIN_TEST_PATH: pathlib.Path = TEST_DATA_PATH / 'train.csv'\n",
    "REVEALED_TARGET_PATH: pathlib.Path = TEST_DATA_PATH / \"revealed_targets.csv\"\n",
    "SAMPLE_SUBMISSION_PATH: pathlib.Path = TEST_DATA_PATH / \"sample_submission.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.356777138Z",
     "start_time": "2024-02-06T12:24:44.296106894Z"
    }
   },
   "id": "34a4296683302084",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77db07498f5b3514"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL_PARAMETER_PATH: pathlib.Path = pathlib.Path(\"/home/paolo/git/enerfit-predict-energy-behaviour/models/models_parameter.toml\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.357456651Z",
     "start_time": "2024-02-06T12:24:44.314894390Z"
    }
   },
   "id": "17a9804f12d1af98",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9999667d83cf4492"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def assert_null_counts(dataframe: pl.LazyFrame) -> bool:\n",
    "    if not DEBUG:\n",
    "        return False\n",
    "    length_dataframe: int = dataframe.collect().shape[0]\n",
    "    \n",
    "    return np.any(dataframe.null_count().with_columns(\n",
    "        *[pl.col(col) == length_dataframe for col in dataframe.columns]\n",
    "    ).collect().to_numpy())\n",
    "\n",
    "\n",
    "\n",
    "def load_parameters(path: pathlib.Path) -> dict[str, str]:\n",
    "    with path.open(\"rb\") as f:\n",
    "        return tomllib.load(f)\n",
    "\n",
    "# config = load_parameters(pathlib.Path(\"/home/paolo/git/enerfit-predict-energy-behaviour/models/models_parameter.toml\"))\n",
    "# config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.358027348Z",
     "start_time": "2024-02-06T12:24:44.314975974Z"
    }
   },
   "id": "123a85b84a147e71",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "334f92653947aada",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c03ba87bf5cff4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load client and convert columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31da8cc56e8c4166",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.358598817Z",
     "start_time": "2024-02-06T12:24:44.315020849Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_clients() -> pl.LazyFrame:\n",
    "    clients: pl.LazyFrame = pl.scan_csv(CLIENTS_PATH)\n",
    "    clients = clients.with_columns(\n",
    "        [\n",
    "            pl.col(\"product_type\").cast(pl.Int8),\n",
    "            pl.col(\"county\").cast(pl.Int8),\n",
    "            pl.col(\"eic_count\").cast(pl.Int16),\n",
    "            pl.col(\"installed_capacity\").cast(pl.Float32),\n",
    "            pl.col(\"is_business\").cast(pl.Int8),\n",
    "            pl.col(\"date\").str.to_date(DATE_FORMAT),\n",
    "            pl.col(\"data_block_id\").cast(pl.Int16)\n",
    "        ]\n",
    "    )\n",
    "    return clients\n",
    "\n",
    "def load_electricity() -> pl.LazyFrame:\n",
    "    electricity: pl.LazyFrame = pl.scan_csv(ELECTRICITY_PATH).drop([\"origin_date\"])\n",
    "    electricity = electricity.with_columns(\n",
    "        [\n",
    "            pl.col(\"forecast_date\").str.to_datetime(DATETIME_FORMAT) + pl.duration(days=1),\n",
    "            pl.col(\"euros_per_mwh\").cast(pl.Float32),\n",
    "            pl.col(\"data_block_id\").cast(pl.Int16)\n",
    "        ]\n",
    "    ).rename({\"forecast_date\": \"datetime\", \"euros_per_mwh\": \"electricity_euros_per_mwh\"})\n",
    "    return electricity\n",
    "\n",
    "\n",
    "def load_gas() -> pl.LazyFrame:\n",
    "    gas: pl.LazyFrame = pl.scan_csv(GAS_PATH).drop([\"origin_date\"])\n",
    "    gas = gas.with_columns(\n",
    "        [\n",
    "            pl.col(\"forecast_date\").str.to_date(DATE_FORMAT),\n",
    "            pl.col(\"lowest_price_per_mwh\").cast(pl.Float32),\n",
    "            pl.col(\"highest_price_per_mwh\").cast(pl.Float32),\n",
    "            ((pl.col(\"lowest_price_per_mwh\") + pl.col(\"highest_price_per_mwh\")) / 2).alias(\"gas_mean_price_per_mhw\"),\n",
    "            pl.col(\"data_block_id\").cast(pl.Int16)\n",
    "        ]\n",
    "    ).rename({\"forecast_date\": \"date\", \"lowest_price_per_mwh\": \"gas_lowest_price_per_mwh\", \"highest_price_per_mwh\": \"gas_highest_price_per_mwh\"})\n",
    "    return gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9b3b2038318184",
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.359170265Z",
     "start_time": "2024-02-06T12:24:44.315081203Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_weather_station_mapping() -> pl.LazyFrame:\n",
    "    weather_station_county_mapping: pl.LazyFrame = pl.scan_csv(WEATHER_STATION_COUNTY_PATH)\n",
    "    weather_station_county_mapping = weather_station_county_mapping.with_columns([\n",
    "        pl.col(\"longitude\").cast(pl.Float32).round(decimals=2),\n",
    "        pl.col(\"latitude\").cast(pl.Float32).round(decimals=2),\n",
    "        pl.col(\"county\").cast(pl.Int8).fill_null(-1),\n",
    "        pl.col(\"county_name\").fill_null(\"Unknown\")\n",
    "    ])\n",
    "    \n",
    "    weather_station_county_mapping = weather_station_county_mapping.join(other=weather_station_county_mapping.group_by(\"county\").agg([\n",
    "        pl.col(\"longitude\").min().alias(\"longitude_min\"),\n",
    "        pl.col(\"longitude\").max().alias(\"longitude_max\"),\n",
    "        pl.col(\"latitude\").min().alias(\"latitude_min\"),\n",
    "        pl.col(\"latitude\").max().alias(\"latitude_max\"),\n",
    "    ]),\n",
    "        on=[\"county\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    return weather_station_county_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98a38c589b3ff23",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load weather forecast and convert columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9925c7c2c7a5de0",
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.359730954Z",
     "start_time": "2024-02-06T12:24:44.315154221Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_weather_forecast(weather_station_county_mapping: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    weather_forecast: pl.LazyFrame = pl.scan_csv(WEATHER_FORECAST_PATH).drop([\"origin_datetime\"]).rename({\"forecast_datetime\": \"datetime\"})\n",
    "    # weather_forecast = weather_forecast.filter(pl.col(\"hours_ahead\") >= 24)  # we don't need to forecast for today\n",
    "    weather_forecast = weather_forecast.with_columns(\n",
    "        [\n",
    "            pl.col(\"datetime\").str.to_datetime(DATETIME_FORMAT),\n",
    "            pl.col(\"latitude\").cast(pl.Float32).round(decimals=2),\n",
    "            pl.col(\"longitude\").cast(pl.Float32).round(decimals=2),\n",
    "            pl.col(\"data_block_id\").cast(pl.Int16),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    weather_forecast = weather_forecast.join(\n",
    "        other=weather_station_county_mapping,\n",
    "        how=\"left\",\n",
    "        on=[\"latitude\", \"longitude\"]\n",
    "    ).drop([\"latitude\", \"longitude\"])\n",
    "\n",
    "    weather_forecast = weather_forecast.group_by(\"county\", \"datetime\", \"data_block_id\").agg(\n",
    "            pl.col(\"hours_ahead\").mean(),\n",
    "            pl.col(\"temperature\").mean(),\n",
    "            pl.col(\"dewpoint\").mean(),\n",
    "            pl.col(\"cloudcover_high\").mean(),\n",
    "            pl.col(\"cloudcover_low\").mean(),\n",
    "            pl.col(\"cloudcover_mid\").mean(),\n",
    "            pl.col(\"cloudcover_total\").mean(),\n",
    "            pl.col(\"10_metre_u_wind_component\").mean(),\n",
    "            pl.col(\"10_metre_v_wind_component\").mean(),\n",
    "            pl.col(\"direct_solar_radiation\").mean(),\n",
    "            pl.col(\"surface_solar_radiation_downwards\").mean(),\n",
    "            pl.col(\"snowfall\").mean(),\n",
    "            pl.col(\"total_precipitation\").mean(),\n",
    "            pl.col(\"latitude_min\").first(),\n",
    "            pl.col(\"latitude_max\").first(),\n",
    "            pl.col(\"longitude_min\").first(),\n",
    "            pl.col(\"longitude_max\").first(),\n",
    "            pl.col(\"county_name\").first(),\n",
    "    )\n",
    "    \n",
    "    return weather_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494168811185b819",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load historical weather and convert columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10fc1bd7878d8537",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.360834968Z",
     "start_time": "2024-02-06T12:24:44.318830021Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_historical_weather(weather_station_county_mapping: pl.LazyFrame) -> pl.LazyFrame:\n",
    "\n",
    "    historical_weather: pl.LazyFrame = pl.scan_csv(HISTORICAL_WEATHER_PATH)\n",
    "    historical_weather = historical_weather.with_columns(\n",
    "        [\n",
    "            pl.col(\"datetime\").str.to_datetime(DATETIME_FORMAT),\n",
    "            pl.col(\"latitude\").cast(pl.Float32).round(decimals=2),\n",
    "            pl.col(\"longitude\").cast(pl.Float32).round(decimals=2),\n",
    "            pl.col(\"data_block_id\").cast(pl.Int16)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    historical_weather = historical_weather.join(\n",
    "        other=weather_station_county_mapping,\n",
    "        how=\"left\",\n",
    "        on=[\"latitude\", \"longitude\"]\n",
    "    ).drop([\"latitude\", \"longitude\"])\n",
    "\n",
    "    historical_weather = historical_weather.group_by(\"county\", \"datetime\", \"data_block_id\").agg(\n",
    "        pl.col(\"temperature\").mean(),\n",
    "        pl.col(\"dewpoint\").mean(),\n",
    "        pl.col(\"rain\").mean(),\n",
    "        pl.col(\"snowfall\").mean(),\n",
    "        pl.col(\"surface_pressure\").mean(),\n",
    "        pl.col(\"cloudcover_total\").mean(),\n",
    "        pl.col(\"cloudcover_low\").mean(),\n",
    "        pl.col(\"cloudcover_mid\").mean(),\n",
    "        pl.col(\"cloudcover_high\").mean(),\n",
    "        pl.col(\"windspeed_10m\").mean(),\n",
    "        pl.col(\"winddirection_10m\").mean(),\n",
    "        pl.col(\"shortwave_radiation\").mean(),\n",
    "        pl.col(\"direct_solar_radiation\").mean(),\n",
    "        pl.col(\"diffuse_radiation\").mean(),\n",
    "        pl.col(\"latitude_min\").first(),\n",
    "        pl.col(\"latitude_max\").first(),\n",
    "        pl.col(\"longitude_min\").first(),\n",
    "        pl.col(\"longitude_max\").first(),\n",
    "        pl.col(\"county_name\").first(),\n",
    "        )\n",
    "    \n",
    "    # Test set has 1 day offset for hour<11 and 2 day offset for hour>11\n",
    "    historical_weather = historical_weather.with_columns(\n",
    "        pl.when(pl.col(\"datetime\").dt.hour() < 11).then(pl.col(\"datetime\") + pl.duration(days=1)).otherwise(pl.col(\"datetime\") + pl.duration(days=2))\n",
    "    )\n",
    "    \n",
    "    return historical_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca239a0a17374998",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Train and convert columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8222293b58141fbb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.398353386Z",
     "start_time": "2024-02-06T12:24:44.325447324Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_train() -> pl.LazyFrame:\n",
    "    train: pl.LazyFrame = pl.scan_csv(TRAIN_PATH)\n",
    "    \n",
    "    train = train.drop([\"prediction_unit_id\", \"row_id\"]).with_columns(\n",
    "            pl.col(\"datetime\").str.to_datetime(DATETIME_FORMAT),\n",
    "            pl.col(\"is_business\").cast(pl.Int8),\n",
    "            pl.col(\"product_type\").cast(pl.Int8),\n",
    "            pl.col(\"target\").cast(pl.Float32),\n",
    "            pl.col(\"is_consumption\").cast(pl.Int8),\n",
    "            pl.col(\"county\").cast(pl.Int8),\n",
    "            pl.col(\"data_block_id\").cast(pl.Int16),\n",
    "    )\n",
    "    train = train.with_columns(\n",
    "            pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "            pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "            pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "            pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "            pl.col(\"datetime\").dt.ordinal_day().alias(\"day_of_year\"),\n",
    "            pl.col(\"datetime\").dt.hour().alias(\"hour\")\n",
    "    )\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create lagged weather features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55af52a87f18af4f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_lagged_weather_forecast(weather_forecast: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    return weather_forecast.sort(\"county\", \"datetime\", \"data_block_id\").rolling(index_column=\"datetime\", period=f\"1d\", by=[\"county\", \"data_block_id\"]).agg(\n",
    "        [\n",
    "            pl.col(\"temperature\").mean().alias(\"temperature_forecast_last_day\"),\n",
    "            pl.col(\"dewpoint\").mean().alias(\"dewpoint_forecast_last_day\"),\n",
    "            pl.col(\"snowfall\").mean().alias(\"snowfall_forecast_last_day\"),\n",
    "            pl.col(\"cloudcover_total\").mean().alias(\"cloudcover_total_forecast_last_day\"),\n",
    "            pl.col(\"cloudcover_low\").mean().alias(\"cloudcover_low_forecast_last_day\"),\n",
    "            pl.col(\"cloudcover_mid\").mean().alias(\"cloudcover_mid_forecast_last_day\"),\n",
    "            pl.col(\"cloudcover_high\").mean().alias(\"cloudcover_high_forecast_last_day\"),\n",
    "            pl.col(\"10_metre_u_wind_component\").mean().alias(\"10_metre_u_wind_component_forecast_last_day\"),\n",
    "            pl.col(\"10_metre_v_wind_component\").mean().alias(\"10_metre_v_wind_component_forecast_last_day\"),\n",
    "            pl.col(\"surface_solar_radiation_downwards\").mean().alias(\"surface_solar_radiation_downwards_forecast_last_day\"),\n",
    "            pl.col(\"direct_solar_radiation\").mean().alias(\"direct_solar_radiation_forecast_last_day\"),\n",
    "            pl.col(\"total_precipitation\").mean().alias(\"total_precipitation_forecast_last_day\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def create_lagged_historical_weather_last_week(historical_weather: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    return historical_weather.sort(\"county\", \"datetime\", \"data_block_id\").rolling(index_column=\"datetime\", period=f\"1w\", by=[\"county\"]).agg(\n",
    "        [\n",
    "            pl.col(\"temperature\").mean().alias(\"temperature_last_7_days\"),\n",
    "            pl.col(\"dewpoint\").mean().alias(\"dewpoint_last_7_days\"),\n",
    "            pl.col(\"rain\").mean().alias(\"rain_last_7_days\"),\n",
    "            pl.col(\"snowfall\").mean().alias(\"snowfall_last_7_days\"),\n",
    "            pl.col(\"cloudcover_total\").mean().alias(\"cloudcover_total_last_7_days\"),\n",
    "            pl.col(\"cloudcover_low\").mean().alias(\"cloudcover_low_last_7_days\"),\n",
    "            pl.col(\"cloudcover_mid\").mean().alias(\"cloudcover_mid_last_7_days\"),\n",
    "            pl.col(\"cloudcover_high\").mean().alias(\"cloudcover_high_last_7_days\"),\n",
    "            pl.col(\"windspeed_10m\").mean().alias(\"windspeed_10m_last_7_days\"),\n",
    "            pl.col(\"winddirection_10m\").mean().alias(\"winddirection_10m_last_7_days\"),\n",
    "            pl.col(\"shortwave_radiation\").mean().alias(\"shortwave_radiation_last_7_days\"),\n",
    "            pl.col(\"direct_solar_radiation\").mean().alias(\"direct_solar_radiation_last_7_days\"),\n",
    "            pl.col(\"diffuse_radiation\").mean().alias(\"diffuse_radiation_last_7_days\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_lagged_historical_weather_last_day(historical_weather: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    return historical_weather.with_columns(pl.col(\"datetime\").dt.hour().alias(\"hour\")).sort(\"county\", \"datetime\").rolling(index_column=\"datetime\", period=f\"1d\", by=[\"county\"]).agg(\n",
    "        [\n",
    "            pl.col(\"temperature\").mean().alias(\"temperature_last_24_hours\"),\n",
    "            pl.col(\"dewpoint\").mean().alias(\"dewpoint_last_24_hours\"),\n",
    "            pl.col(\"rain\").mean().alias(\"rain_last_24_hours\"),\n",
    "            pl.col(\"snowfall\").mean().alias(\"snowfall_last_24_hours\"),\n",
    "            pl.col(\"cloudcover_total\").mean().alias(\"cloudcover_total_last_24_hours\"),\n",
    "            pl.col(\"cloudcover_low\").mean().alias(\"cloudcover_low_last_24_hours\"),\n",
    "            pl.col(\"cloudcover_mid\").mean().alias(\"cloudcover_mid_last_24_hours\"),\n",
    "            pl.col(\"cloudcover_high\").mean().alias(\"cloudcover_high_last_24_hours\"),\n",
    "            pl.col(\"windspeed_10m\").mean().alias(\"windspeed_10m_last_24_hours\"),\n",
    "            pl.col(\"winddirection_10m\").mean().alias(\"winddirection_10m_last_24_hours\"),\n",
    "            pl.col(\"shortwave_radiation\").mean().alias(\"shortwave_radiation_last_24_hours\"),\n",
    "            pl.col(\"direct_solar_radiation\").mean().alias(\"direct_solar_radiation_last_24_hours\"),\n",
    "            pl.col(\"diffuse_radiation\").mean().alias(\"diffuse_radiation_last_24_hours\"),\n",
    "        ]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.398997852Z",
     "start_time": "2024-02-06T12:24:44.370883688Z"
    }
   },
   "id": "a82404e95a9711d3",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def add_clients_id(data: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    client_ids_columns = [\"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n",
    "\n",
    "    data = (data\n",
    "    .group_by(client_ids_columns).len()\n",
    "    .drop(\"len\")\n",
    "    .sort(client_ids_columns)\n",
    "    .with_row_index(name=\"client_id\")\n",
    "    .join(\n",
    "        other=data,\n",
    "        how=\"inner\",\n",
    "        on=client_ids_columns\n",
    "    ))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data() -> tuple[pl.LazyFrame, ...]:\n",
    "    train_data: pl.LazyFrame = load_train()\n",
    "    gas_data: pl.LazyFrame = load_gas()\n",
    "    electricity_data: pl.LazyFrame = load_electricity()\n",
    "    clients_data: pl.LazyFrame = load_clients()\n",
    "    weather_county_map: pl.LazyFrame = load_weather_station_mapping()\n",
    "    weather_forecast_data: pl.LazyFrame = load_weather_forecast(weather_county_map)\n",
    "    historical_weather_data: pl.LazyFrame = load_historical_weather(weather_county_map)\n",
    "    return train_data, gas_data, electricity_data, clients_data, weather_forecast_data, historical_weather_data\n",
    "\n",
    "def add_monthly_historical_data(data: pl.LazyFrame, historical_weather_data: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    columns_to_avg = list(set(historical_weather_data.columns).difference([\"county\", \"datetime\", \"data_block_id\", \"latitude_min\", \"latitude_max\", \"longitude_min\", \"longitude_max\", \"county_name\"]))\n",
    "    monthly_average_data: list[pl.LazyFrame] = []\n",
    "\n",
    "    historical_weather_data = historical_weather_data.with_columns(\n",
    "        pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "    )\n",
    "\n",
    "    for year, month in historical_weather_data.select(\"year\", \"month\").unique().collect().iter_rows():\n",
    "        temp_df = historical_weather_data.filter(\n",
    "            (pl.col(\"year\") < year) &\n",
    "            (pl.col(\"month\") == month)\n",
    "        )\n",
    "    \n",
    "        if temp_df.collect().is_empty():\n",
    "            continue\n",
    "    \n",
    "        # get historical monthly average\n",
    "        temp_df = temp_df.group_by(\"month\").agg(\n",
    "            *[pl.col(column).mean().alias(f\"{column}_monthly_historical\") for column in columns_to_avg]\n",
    "        )\n",
    "    \n",
    "        # add year column\n",
    "        monthly_average_data.append(temp_df.with_columns(pl.lit(year).alias(\"year\")))\n",
    "    \n",
    "        # print(year, month)\n",
    "    \n",
    "    monthly_average_data: pl.LazyFrame = pl.concat(monthly_average_data)\n",
    "    \n",
    "    # join with train data\n",
    "    data = data.join(monthly_average_data, on=[\"year\", \"month\"], how=\"left\")\n",
    "    \n",
    "    # fill null values with original values\n",
    "    return data.with_columns(\n",
    "        *[pl.col(f\"{column}_monthly_historical\").fill_null(pl.col(column)) for column in columns_to_avg]\n",
    "    )\n",
    "    \n",
    "\n",
    "def create_dataset() -> pl.LazyFrame:\n",
    "    train_data, gas_data, electricity_data, clients_data, weather_forecast_data, historical_weather_data = load_data()\n",
    "    \n",
    "    data: pl.LazyFrame = (train_data\n",
    "                            .join(other=clients_data, how=\"left\", on=[\"county\", \"is_business\", \"product_type\", \"data_block_id\"], suffix=\"_client\")\n",
    "                            .join(other=gas_data, on=\"data_block_id\", how=\"left\", suffix=\"_gas\")\n",
    "                            .join(other=electricity_data, on=[\"datetime\", \"data_block_id\"], how=\"left\", suffix=\"_electricity\")\n",
    "                            )\n",
    "    \n",
    "    data = add_clients_id(data)\n",
    "    \n",
    "    data = (data\n",
    "            .join(other=historical_weather_data, how=\"left\", on=[\"county\", \"datetime\", \"data_block_id\"], suffix=\"_measured\")\n",
    "            .join(other=weather_forecast_data, how=\"left\", on=[\"county\", \"datetime\", \"data_block_id\"], suffix=\"_forecast\")\n",
    "            )\n",
    "    \n",
    "    data = (data\n",
    "              .join(other=create_lagged_weather_forecast(weather_forecast_data), on=[\"county\", \"datetime\", \"data_block_id\"], how=\"left\")\n",
    "              .join(other=create_lagged_historical_weather_last_week(historical_weather_data), on=[\"county\", \"datetime\"], how=\"left\")\n",
    "              .join(other=create_lagged_historical_weather_last_day(historical_weather_data), on=[\"county\", \"datetime\"], how=\"left\")\n",
    "              )\n",
    "    \n",
    "    # data = add_monthly_historical_data(data, historical_weather_data)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.411798188Z",
     "start_time": "2024-02-06T12:24:44.380718886Z"
    }
   },
   "id": "4d41ef2ee0ac1933",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53d78c03637fc71e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_revealed_target_features(data: pl.LazyFrame, lag_days: int = 7) -> pl.LazyFrame:    \n",
    "    revealed_targets = data.select(\"datetime\", \"client_id\", \"target\")\n",
    "    \n",
    "    # Create revealed targets for all day lags\n",
    "    for day_lag in range(2, lag_days+1):\n",
    "        data = data.join(revealed_targets.with_columns(pl.col(\"datetime\") + pl.duration(days=day_lag)),\n",
    "                          how=\"left\",\n",
    "                          on = [\"datetime\", \"client_id\"],\n",
    "                          suffix = f'_{day_lag}_days_ago'\n",
    "                          )\n",
    "    return data\n",
    "\n",
    "def create_time_based_features(data: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    return data.with_columns(\n",
    "        (2 * np.pi * pl.col(\"hour\") / 24).sin().cast(pl.Float32).alias(\"sin(hour)\"),\n",
    "        (2 * np.pi * pl.col(\"hour\") / 24).cos().cast(pl.Float32).alias(\"cos(hour)\"),\n",
    "        (2 * np.pi * pl.col(\"weekday\") / 7).sin().cast(pl.Float32).alias(\"sin(weekday)\"),\n",
    "        (2 * np.pi * pl.col(\"weekday\") / 7).cos().cast(pl.Float32).alias(\"cos(weekday)\"),\n",
    "        (2 * np.pi * pl.col(\"month\") / 12).sin().cast(pl.Float32).alias(\"sin(month)\"),\n",
    "        (2 * np.pi * pl.col(\"month\") / 12).cos().cast(pl.Float32).alias(\"cos(month)\"),\n",
    "        pl.when(pl.col(\"datetime\").dt.is_leap_year()).then(np.pi * pl.col(\"day_of_year\") / 366).otherwise(np.pi * pl.col(\"day_of_year\") / 365).sin().cast(pl.Float32).alias(\"sin(day_of_year)\"),\n",
    "        pl.when(pl.col(\"datetime\").dt.is_leap_year()).then(np.pi * pl.col(\"day_of_year\") / 366).otherwise(np.pi * pl.col(\"day_of_year\") / 365).cos().cast(pl.Float32).alias(\"cos(day_of_year)\"),\n",
    "        # pl.col(\"datetime\").dt.quarter().alias(\"quarter\"),\n",
    "        pl.col(\"date\").dt.strftime(\"%Y-%m-%d\").is_in([x.strftime(\"%Y-%m-%d\") for x  in ESTONIAN_HOLIDAYS]).alias(\"is_holiday\"),\n",
    "    ).drop([\"hour\", \"day_of_year\", \"weekday\"])\n",
    "\n",
    "def map_product_type_to_string_values(data: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    product_type_map: dict[int, str] = {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}\n",
    "\n",
    "    return data.with_columns(\n",
    "        pl.col(\"product_type\").replace(product_type_map, default=\"Unknown\")\n",
    "    )\n",
    "\n",
    "def cast_data_to_32_bits(data: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    return data.with_columns(\n",
    "        pl.col(pl.Int64).cast(pl.Int32),\n",
    "        pl.col(pl.Float64).cast(pl.Float32),\n",
    "    )\n",
    "\n",
    "def add_noise_feature_for_training(data: pl.LazyFrame) -> pl.DataFrame:\n",
    "    data = data.collect()\n",
    "    return data.with_columns(\n",
    "        pl.lit(np.random.normal(0, 1, size=data.shape[0])).alias(\"noise\")\n",
    "    )\n",
    "\n",
    "def feature_engineer(data: pl.LazyFrame) -> pl.DataFrame:\n",
    "    data = create_revealed_target_features(data, lag_days=7)\n",
    "    data = create_time_based_features(data)\n",
    "    data = map_product_type_to_string_values(data)\n",
    "    data = cast_data_to_32_bits(data)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.412488261Z",
     "start_time": "2024-02-06T12:24:44.387586751Z"
    }
   },
   "id": "e88e0b7bf3366689",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_objects_columns_to_category(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype == \"object\":\n",
    "            dataset[col] = dataset[col].astype(\"category\")\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.439632264Z",
     "start_time": "2024-02-06T12:24:44.391563539Z"
    }
   },
   "id": "684baa549890e71f",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "4d626e773dc4be54",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train model function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254925dd10e59e4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91dc972823131604",
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.441431871Z",
     "start_time": "2024-02-06T12:24:44.396535474Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation_month_and_year(dataframe: pd.DataFrame, train_months: int = 3, test_months: int = 1, debug: bool = False) -> Generator:\n",
    "    for _, row in dataframe[[\"year\", \"month\"]].drop_duplicates().sort_values([\"year\", \"month\"]).iterrows():\n",
    "        current_date = datetime.date(row.year, row.month, 1) + relativedelta(months=1)\n",
    "        start_train_date = current_date - relativedelta(months=train_months)\n",
    "        end_train_date = current_date\n",
    "        start_test_date = current_date\n",
    "        end_test_date = current_date + relativedelta(months=test_months)\n",
    "\n",
    "        try:\n",
    "            train_index = dataframe.query(\"(date >= @start_train_date & date < @end_train_date) | (year < @row.year & month == @row.month)\").index\n",
    "            test_index = dataframe.query(\"(date >= @start_test_date & date < @end_test_date)\").index\n",
    "        except KeyError as e:\n",
    "            continue\n",
    "\n",
    "        if debug:\n",
    "            print(f\"train date: {start_train_date} - {end_train_date-relativedelta(months=1)}\\ntest date: {start_test_date} - {end_test_date-relativedelta(months=1)}\")\n",
    "\n",
    "        if len(train_index) == 0 or len(test_index) == 0:\n",
    "            continue\n",
    "\n",
    "        yield train_index, test_index, start_train_date, end_train_date, start_test_date, end_test_date\n",
    "\n",
    "def split_train_test(data: pd.DataFrame, test_months: int = 6) -> tuple[list[int], ...]:\n",
    "    max_dataset_date: datetime.date = data[\"date\"].max()\n",
    "    start_test_date: datetime.date = max_dataset_date - relativedelta(months=test_months)\n",
    "    train_index = data.query(\"date < @start_test_date\").index\n",
    "    test_index = data.query(\"date >= @start_test_date\").index\n",
    "    return train_index, test_index\n",
    "\n",
    "def train_model_cross_validation(dataframe: pd.DataFrame) -> list[lgb.LGBMRegressor]:\n",
    "    models = []\n",
    "    for i, (train_index, test_index, start_train_date, end_train_date, start_test_date, end_test_date) in enumerate(cross_validation_month_and_year(dataframe, train_months=15, test_months=6, debug=False)):\n",
    "        print(f\"Split {i+1} - train from {start_train_date} to {end_train_date} --- test from {start_test_date} to {end_test_date}\\n\\n\\n\")\n",
    "        models.append(train_model(dataframe, train_index, test_index, debug=False))\n",
    "        break\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_data_and_train_model(columns_to_drop: list[str], model_type: str, objective: str = \"l2\") -> lgb.LGBMRegressor:\n",
    "    model_type = model_type.lower()\n",
    "    \n",
    "    if model_type not in [\"producer\", \"consumer\"]:\n",
    "        raise ValueError(f\"Model type must be either 'producer' or 'consumer', given model type is: {model_type}\")\n",
    "    \n",
    "    data = create_dataset()\n",
    "    data = data.filter(pl.col(\"is_consumption\") == MAP_MODEL_TYPE.get(model_type, 0)).drop([\"is_consumption\"])\n",
    "    data = feature_engineer(data)\n",
    "    data = data.drop_nulls()\n",
    "    data = data.drop(columns_to_drop)\n",
    "    data = add_noise_feature_for_training(data).to_pandas()\n",
    "    data = convert_objects_columns_to_category(data)\n",
    "\n",
    "    train_index, test_index = split_train_test(data=data)\n",
    "    data = data.drop(columns=[\"date\"])\n",
    "\n",
    "    model = train_model(dataframe=data, train_indexes=train_index, test_indexes=test_index, objective=objective)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.446787050Z",
     "start_time": "2024-02-06T12:24:44.399391866Z"
    }
   },
   "id": "f75c2ff588267571",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_feature_importances_and_print_useless_columns(model: lgb.LGBMRegressor) -> pd.DataFrame:\n",
    "    feature_importances = pd.DataFrame({\"feature\": model.feature_name_, \"importance\": model.feature_importances_}).sort_values(\"importance\", ascending=False)\n",
    "    feature_importances[\"importance_perc\"] = (feature_importances[\"importance\"] / feature_importances[\"importance\"].sum()) * 100\n",
    "    feature_importances = feature_importances.sort_values(\"importance_perc\", ascending=False).reset_index(drop=True)\n",
    "    feature_importances[\"importance_perc_cumulative\"] = feature_importances[\"importance_perc\"].cumsum()\n",
    "\n",
    "    noise_importance = feature_importances.query(\"feature == 'noise'\").importance.item()\n",
    "\n",
    "    print(feature_importances[feature_importances[\"importance\"] < noise_importance].feature.tolist())\n",
    "\n",
    "    return feature_importances"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.447778672Z",
     "start_time": "2024-02-06T12:24:44.409915805Z"
    }
   },
   "id": "a6c743c6172a20fa",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(dataframe: pd.DataFrame, train_indexes: list[int], test_indexes: list[int], objective: str = \"l2\") -> tuple:    \n",
    "    x_train, x_test = dataframe.loc[train_indexes], dataframe.loc[test_indexes]\n",
    "    y_train, y_test = x_train.pop(\"target\"), x_test.pop(\"target\")\n",
    "    eval_results = {}\n",
    "    model = lgb.LGBMRegressor(\n",
    "        boosting_type='gbdt', \n",
    "        num_leaves=31, \n",
    "        max_depth=-1, \n",
    "        learning_rate=0.1, \n",
    "        n_estimators=10_000, \n",
    "        subsample_for_bin=200000, \n",
    "        objective=objective, \n",
    "        min_split_gain=0.0, \n",
    "        min_child_weight=0.001, \n",
    "        min_child_samples=20, \n",
    "        subsample=1.0, \n",
    "        subsample_freq=0, \n",
    "        colsample_bytree=1.0, \n",
    "        reg_alpha=0.0, \n",
    "        reg_lambda=0.0, \n",
    "        random_state=SEED, \n",
    "        n_jobs=-1, \n",
    "        importance_type='split',\n",
    "        linear_tree = True,\n",
    "        verbosity = 0,\n",
    "        device = \"cpu\",\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X=x_train, \n",
    "        y=y_train,\n",
    "        eval_set=[(x_test, y_test)],\n",
    "        eval_metric=\"mae\",\n",
    "        callbacks=[\n",
    "            lgb.log_evaluation(), \n",
    "            lgb.record_evaluation(eval_results), \n",
    "            lgb.early_stopping(stopping_rounds=100)\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.448625682Z",
     "start_time": "2024-02-06T12:24:44.409983282Z"
    }
   },
   "id": "652821edf3e8b931",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train consumer model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c769a5a4d5ca8436"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %%time \n",
    "# \n",
    "# columns_to_drop_producer = [\"client_id\",\n",
    "#                    \"data_block_id\",\n",
    "#                    # \"date\",\n",
    "#                    \"date_client\",\n",
    "#                    \"date_gas\",\n",
    "#                    \"datetime\",\n",
    "#                    \"county\",\n",
    "#                    \"county_name_forecast\",\n",
    "#                    \"latitude_min_forecast\",\n",
    "#                    \"latitude_max_forecast\",\n",
    "#                    \"longitude_min_forecast\",\n",
    "#                    \"longitude_max_forecast\",\n",
    "#                    # \"hour\",\n",
    "#                    # \"quarter\",\n",
    "#                    \"year\",\n",
    "#                    \"month\"]\n",
    "# \n",
    "# # columns_to_drop = columns_to_drop + ['direct_solar_radiation_last_24_hours',\n",
    "# # 'cloudcover_low_forecast_last_day',\n",
    "# # 'winddirection_10m_last_24_hours',\n",
    "# # 'snowfall_forecast_last_day',\n",
    "# # '10_metre_u_wind_component',\n",
    "# # 'product_type',\n",
    "# # 'sin(day_of_year)',\n",
    "# # 'rain_last_7_days',\n",
    "# # 'surface_solar_radiation_downwards_forecast_last_day',\n",
    "# # 'cloudcover_high_forecast_last_day',\n",
    "# # 'gas_lowest_price_per_mwh',\n",
    "# # 'cloudcover_high_last_7_days',\n",
    "# # 'temperature_last_24_hours',\n",
    "# # 'cloudcover_low',\n",
    "# # 'dewpoint_forecast',\n",
    "# # 'shortwave_radiation',\n",
    "# # 'cos(day_of_year)',\n",
    "# # 'dewpoint_forecast_last_day',\n",
    "# # 'electricity_euros_per_mwh',\n",
    "# # '10_metre_v_wind_component',\n",
    "# # 'cos(hour)',\n",
    "# # 'winddirection_10m',\n",
    "# # 'longitude_max',\n",
    "# # 'longitude_min',\n",
    "# # 'is_holiday',\n",
    "# # 'shortwave_radiation_last_24_hours',\n",
    "# # 'cloudcover_low_last_24_hours',\n",
    "# # 'cloudcover_mid_last_24_hours',\n",
    "# # 'shortwave_radiation_last_7_days',\n",
    "# # 'temperature',\n",
    "# # 'snowfall_last_7_days',\n",
    "# # 'day',\n",
    "# # 'sin(hour)',\n",
    "# # 'temperature_forecast_last_day',\n",
    "# # 'dewpoint_last_7_days',\n",
    "# # 'surface_pressure',\n",
    "# # 'temperature_forecast',\n",
    "# # 'cloudcover_total_last_24_hours',\n",
    "# # 'snowfall_last_24_hours',\n",
    "# # 'rain_last_24_hours',\n",
    "# # 'diffuse_radiation_last_7_days',\n",
    "# # 'cloudcover_high',\n",
    "# # 'winddirection_10m_last_7_days',\n",
    "# # 'windspeed_10m_last_7_days',\n",
    "# # 'windspeed_10m',\n",
    "# # 'direct_solar_radiation',\n",
    "# # 'temperature_last_7_days',\n",
    "# # 'diffuse_radiation',\n",
    "# # 'latitude_min',\n",
    "# # 'latitude_max',\n",
    "# # 'dewpoint',\n",
    "# # 'cloudcover_high_forecast',\n",
    "# # 'cloudcover_mid_forecast',\n",
    "# # 'cloudcover_total_forecast',\n",
    "# # 'cloudcover_high_last_24_hours',\n",
    "# # 'windspeed_10m_last_24_hours',\n",
    "# # 'dewpoint_last_24_hours',\n",
    "# # 'snowfall_forecast',\n",
    "# # 'gas_mean_price_per_mhw',\n",
    "# # 'sin(month)',\n",
    "# # 'rain',\n",
    "# # 'cos(month)',\n",
    "# # 'snowfall',\n",
    "# # 'diffuse_radiation_last_24_hours',\n",
    "# # 'cloudcover_mid']\n",
    "# \n",
    "# # best model huber = 48.1918, 15 leaves, 10m training\n",
    "# # best model poisson = 44.699, 15 leaves, 12s training\n",
    "# producer_model = load_data_and_train_model(model_type=\"producer\", columns_to_drop=columns_to_drop_producer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.449555287Z",
     "start_time": "2024-02-06T12:24:44.410013289Z"
    }
   },
   "id": "fa84b7021d327c0e",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# feature_importances_producer = get_feature_importances_and_print_useless_columns(producer_model)\n",
    "# feature_importances_producer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:44.450436320Z",
     "start_time": "2024-02-06T12:24:44.410046371Z"
    }
   },
   "id": "ab2b449760d36eb1",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train consumer model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "218c90f6c1fc4afd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l1: 511.074\tvalid_0's l2: 1.56652e+06\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's l1: 462.069\tvalid_0's l2: 1.28588e+06\n",
      "[3]\tvalid_0's l1: 418.076\tvalid_0's l2: 1.0559e+06\n",
      "[4]\tvalid_0's l1: 378.56\tvalid_0's l2: 869409\n",
      "[5]\tvalid_0's l1: 343.426\tvalid_0's l2: 717767\n",
      "[6]\tvalid_0's l1: 312.122\tvalid_0's l2: 594979\n",
      "[7]\tvalid_0's l1: 284.107\tvalid_0's l2: 493786\n",
      "[8]\tvalid_0's l1: 259.04\tvalid_0's l2: 410345\n",
      "[9]\tvalid_0's l1: 236.795\tvalid_0's l2: 342774\n",
      "[10]\tvalid_0's l1: 217.01\tvalid_0's l2: 288302\n",
      "[11]\tvalid_0's l1: 199.411\tvalid_0's l2: 244457\n",
      "[12]\tvalid_0's l1: 183.948\tvalid_0's l2: 208510\n",
      "[13]\tvalid_0's l1: 171.402\tvalid_0's l2: 190712\n",
      "[14]\tvalid_0's l1: 159.42\tvalid_0's l2: 167445\n",
      "[15]\tvalid_0's l1: 149.164\tvalid_0's l2: 148649\n",
      "[16]\tvalid_0's l1: 140.028\tvalid_0's l2: 133859\n",
      "[17]\tvalid_0's l1: 133.707\tvalid_0's l2: 128627\n",
      "[18]\tvalid_0's l1: 128.835\tvalid_0's l2: 134395\n",
      "[19]\tvalid_0's l1: 123.017\tvalid_0's l2: 127316\n",
      "[20]\tvalid_0's l1: 118.728\tvalid_0's l2: 132679\n",
      "[21]\tvalid_0's l1: 120.823\tvalid_0's l2: 159711\n",
      "[22]\tvalid_0's l1: 114.834\tvalid_0's l2: 153360\n",
      "[23]\tvalid_0's l1: 111.563\tvalid_0's l2: 150181\n",
      "[24]\tvalid_0's l1: 147.747\tvalid_0's l2: 691081\n",
      "[25]\tvalid_0's l1: 144.585\tvalid_0's l2: 686842\n",
      "[26]\tvalid_0's l1: 141.944\tvalid_0's l2: 682819\n",
      "[27]\tvalid_0's l1: 141.372\tvalid_0's l2: 690470\n",
      "[28]\tvalid_0's l1: 139.559\tvalid_0's l2: 687614\n",
      "[29]\tvalid_0's l1: 137.945\tvalid_0's l2: 684260\n",
      "[30]\tvalid_0's l1: 136.354\tvalid_0's l2: 680647\n",
      "[31]\tvalid_0's l1: 135.759\tvalid_0's l2: 683017\n",
      "[32]\tvalid_0's l1: 134.656\tvalid_0's l2: 682454\n",
      "[33]\tvalid_0's l1: 133.504\tvalid_0's l2: 680515\n",
      "[34]\tvalid_0's l1: 135.378\tvalid_0's l2: 658387\n",
      "[35]\tvalid_0's l1: 134.625\tvalid_0's l2: 656575\n",
      "[36]\tvalid_0's l1: 134.04\tvalid_0's l2: 651817\n",
      "[37]\tvalid_0's l1: 136.9\tvalid_0's l2: 709168\n",
      "[38]\tvalid_0's l1: 136.221\tvalid_0's l2: 707945\n",
      "[39]\tvalid_0's l1: 135.66\tvalid_0's l2: 706586\n",
      "[40]\tvalid_0's l1: 136.468\tvalid_0's l2: 713978\n",
      "[41]\tvalid_0's l1: 136.073\tvalid_0's l2: 712794\n",
      "[42]\tvalid_0's l1: 136.181\tvalid_0's l2: 713083\n",
      "[43]\tvalid_0's l1: 135.994\tvalid_0's l2: 712070\n",
      "[44]\tvalid_0's l1: 135.706\tvalid_0's l2: 710859\n",
      "[45]\tvalid_0's l1: 135.3\tvalid_0's l2: 709326\n",
      "[46]\tvalid_0's l1: 135.028\tvalid_0's l2: 707854\n",
      "[47]\tvalid_0's l1: 134.416\tvalid_0's l2: 707022\n",
      "[48]\tvalid_0's l1: 134.206\tvalid_0's l2: 704075\n",
      "[49]\tvalid_0's l1: 134.301\tvalid_0's l2: 704133\n",
      "[50]\tvalid_0's l1: 134.143\tvalid_0's l2: 703521\n",
      "[51]\tvalid_0's l1: 134.341\tvalid_0's l2: 704552\n",
      "[52]\tvalid_0's l1: 134.202\tvalid_0's l2: 703484\n",
      "[53]\tvalid_0's l1: 134.234\tvalid_0's l2: 704668\n",
      "[54]\tvalid_0's l1: 133.882\tvalid_0's l2: 704696\n",
      "[55]\tvalid_0's l1: 133.821\tvalid_0's l2: 705390\n",
      "[56]\tvalid_0's l1: 133.576\tvalid_0's l2: 704795\n",
      "[57]\tvalid_0's l1: 133.441\tvalid_0's l2: 706495\n",
      "[58]\tvalid_0's l1: 133.256\tvalid_0's l2: 706448\n",
      "[59]\tvalid_0's l1: 133.183\tvalid_0's l2: 706257\n",
      "[60]\tvalid_0's l1: 133.036\tvalid_0's l2: 706019\n",
      "[61]\tvalid_0's l1: 133.009\tvalid_0's l2: 705953\n",
      "[62]\tvalid_0's l1: 132.865\tvalid_0's l2: 705902\n",
      "[63]\tvalid_0's l1: 165.771\tvalid_0's l2: 1.112e+06\n",
      "[64]\tvalid_0's l1: 165.832\tvalid_0's l2: 1.11178e+06\n",
      "[65]\tvalid_0's l1: 166.812\tvalid_0's l2: 1.11381e+06\n",
      "[66]\tvalid_0's l1: 168.211\tvalid_0's l2: 1.11629e+06\n",
      "[67]\tvalid_0's l1: 168.32\tvalid_0's l2: 1.11597e+06\n",
      "[68]\tvalid_0's l1: 168.552\tvalid_0's l2: 1.11694e+06\n",
      "[69]\tvalid_0's l1: 184.561\tvalid_0's l2: 1.23977e+06\n",
      "[70]\tvalid_0's l1: 184.647\tvalid_0's l2: 1.2415e+06\n",
      "[71]\tvalid_0's l1: 186.558\tvalid_0's l2: 1.24814e+06\n",
      "[72]\tvalid_0's l1: 206.873\tvalid_0's l2: 1.60488e+06\n",
      "[73]\tvalid_0's l1: 206.731\tvalid_0's l2: 1.60411e+06\n",
      "[74]\tvalid_0's l1: 206.68\tvalid_0's l2: 1.60384e+06\n",
      "[75]\tvalid_0's l1: 206.664\tvalid_0's l2: 1.60255e+06\n",
      "[76]\tvalid_0's l1: 207.317\tvalid_0's l2: 1.60286e+06\n",
      "[77]\tvalid_0's l1: 207.375\tvalid_0's l2: 1.60316e+06\n",
      "[78]\tvalid_0's l1: 207.43\tvalid_0's l2: 1.60444e+06\n",
      "[79]\tvalid_0's l1: 207.341\tvalid_0's l2: 1.60447e+06\n",
      "[80]\tvalid_0's l1: 207.763\tvalid_0's l2: 1.60493e+06\n",
      "[81]\tvalid_0's l1: 207.965\tvalid_0's l2: 1.60475e+06\n",
      "[82]\tvalid_0's l1: 208.053\tvalid_0's l2: 1.60699e+06\n",
      "[83]\tvalid_0's l1: 208.116\tvalid_0's l2: 1.60778e+06\n",
      "[84]\tvalid_0's l1: 208.064\tvalid_0's l2: 1.60781e+06\n",
      "[85]\tvalid_0's l1: 208.254\tvalid_0's l2: 1.60721e+06\n",
      "[86]\tvalid_0's l1: 208.417\tvalid_0's l2: 1.60708e+06\n",
      "[87]\tvalid_0's l1: 208.326\tvalid_0's l2: 1.60717e+06\n",
      "[88]\tvalid_0's l1: 208.996\tvalid_0's l2: 1.59943e+06\n",
      "[89]\tvalid_0's l1: 208.996\tvalid_0's l2: 1.60029e+06\n",
      "[90]\tvalid_0's l1: 208.932\tvalid_0's l2: 1.60008e+06\n",
      "[91]\tvalid_0's l1: 208.885\tvalid_0's l2: 1.59965e+06\n",
      "[92]\tvalid_0's l1: 208.998\tvalid_0's l2: 1.60136e+06\n",
      "[93]\tvalid_0's l1: 209.596\tvalid_0's l2: 1.60278e+06\n",
      "[94]\tvalid_0's l1: 215.298\tvalid_0's l2: 1.63195e+06\n",
      "[95]\tvalid_0's l1: 215.886\tvalid_0's l2: 1.6306e+06\n",
      "[96]\tvalid_0's l1: 222.262\tvalid_0's l2: 1.7172e+06\n",
      "[97]\tvalid_0's l1: 222.31\tvalid_0's l2: 1.71854e+06\n",
      "[98]\tvalid_0's l1: 222.272\tvalid_0's l2: 1.71875e+06\n",
      "[99]\tvalid_0's l1: 222.683\tvalid_0's l2: 1.72029e+06\n",
      "[100]\tvalid_0's l1: 222.82\tvalid_0's l2: 1.72021e+06\n",
      "[101]\tvalid_0's l1: 222.797\tvalid_0's l2: 1.7195e+06\n",
      "[102]\tvalid_0's l1: 236.226\tvalid_0's l2: 2.11763e+06\n",
      "[103]\tvalid_0's l1: 237.294\tvalid_0's l2: 2.11998e+06\n",
      "[104]\tvalid_0's l1: 238.11\tvalid_0's l2: 2.12594e+06\n",
      "[105]\tvalid_0's l1: 237.987\tvalid_0's l2: 2.12583e+06\n",
      "[106]\tvalid_0's l1: 237.967\tvalid_0's l2: 2.12615e+06\n",
      "[107]\tvalid_0's l1: 245.698\tvalid_0's l2: 2.25495e+06\n",
      "[108]\tvalid_0's l1: 245.692\tvalid_0's l2: 2.25517e+06\n",
      "[109]\tvalid_0's l1: 245.728\tvalid_0's l2: 2.25509e+06\n",
      "[110]\tvalid_0's l1: 252.829\tvalid_0's l2: 2.44302e+06\n",
      "[111]\tvalid_0's l1: 253.126\tvalid_0's l2: 2.44502e+06\n",
      "[112]\tvalid_0's l1: 253.131\tvalid_0's l2: 2.44536e+06\n",
      "[113]\tvalid_0's l1: 253.164\tvalid_0's l2: 2.4381e+06\n",
      "[114]\tvalid_0's l1: 252.992\tvalid_0's l2: 2.43515e+06\n",
      "[115]\tvalid_0's l1: 252.965\tvalid_0's l2: 2.43512e+06\n",
      "[116]\tvalid_0's l1: 253.003\tvalid_0's l2: 2.43499e+06\n",
      "[117]\tvalid_0's l1: 253.454\tvalid_0's l2: 2.43577e+06\n",
      "[118]\tvalid_0's l1: 253.442\tvalid_0's l2: 2.43579e+06\n",
      "[119]\tvalid_0's l1: 253.444\tvalid_0's l2: 2.43546e+06\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's l1: 123.017\tvalid_0's l2: 127316\n",
      "CPU times: user 1min 55s, sys: 7.25 s, total: 2min 2s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "columns_to_drop_consumer = [\n",
    "    \"client_id\",\n",
    "    \"data_block_id\",\n",
    "    # \"date\",\n",
    "    \"date_client\",\n",
    "    \"date_gas\",\n",
    "    \"datetime\",\n",
    "    \"county\",\n",
    "    \"county_name_forecast\",\n",
    "    \"latitude_min_forecast\",\n",
    "    \"latitude_max_forecast\",\n",
    "    \"longitude_min_forecast\",\n",
    "    \"longitude_max_forecast\",\n",
    "    # \"hour\",\n",
    "    # \"quarter\",\n",
    "    \"year\",\n",
    "    \"month\"\n",
    "]\n",
    "\n",
    "consumer_model = load_data_and_train_model(model_type=\"consumer\", columns_to_drop=columns_to_drop_consumer, objective=\"l2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:59.263429154Z",
     "start_time": "2024-02-06T12:24:44.412037400Z"
    }
   },
   "id": "41d9bb83834d5580",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            feature  importance  importance_perc  \\\n0                 target_7_days_ago         145        25.438596   \n1                 target_2_days_ago          62        10.877193   \n2   direct_solar_radiation_forecast          37         6.491228   \n3                         cos(hour)          33         5.789474   \n4                        is_holiday          30         5.263158   \n..                              ...         ...              ...   \n90                winddirection_10m           0         0.000000   \n91                              day           0         0.000000   \n92           gas_mean_price_per_mhw           0         0.000000   \n93        electricity_euros_per_mwh           0         0.000000   \n94                            noise           0         0.000000   \n\n    importance_perc_cumulative  \n0                    25.438596  \n1                    36.315789  \n2                    42.807018  \n3                    48.596491  \n4                    53.859649  \n..                         ...  \n90                  100.000000  \n91                  100.000000  \n92                  100.000000  \n93                  100.000000  \n94                  100.000000  \n\n[95 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n      <th>importance_perc</th>\n      <th>importance_perc_cumulative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>target_7_days_ago</td>\n      <td>145</td>\n      <td>25.438596</td>\n      <td>25.438596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>target_2_days_ago</td>\n      <td>62</td>\n      <td>10.877193</td>\n      <td>36.315789</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>direct_solar_radiation_forecast</td>\n      <td>37</td>\n      <td>6.491228</td>\n      <td>42.807018</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cos(hour)</td>\n      <td>33</td>\n      <td>5.789474</td>\n      <td>48.596491</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>is_holiday</td>\n      <td>30</td>\n      <td>5.263158</td>\n      <td>53.859649</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>winddirection_10m</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>day</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>gas_mean_price_per_mhw</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>electricity_euros_per_mwh</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>noise</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>100.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>95 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances_consumer = get_feature_importances_and_print_useless_columns(consumer_model)\n",
    "feature_importances_consumer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T12:24:59.274938080Z",
     "start_time": "2024-02-06T12:24:59.264717446Z"
    }
   },
   "id": "a2aa6a3a6d108ec4",
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
